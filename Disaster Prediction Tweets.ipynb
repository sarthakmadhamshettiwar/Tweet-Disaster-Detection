{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908ade2d",
   "metadata": {},
   "source": [
    "### Importing all the necessary libraries of NLP, keras, ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104361ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c18585cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f41669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing tf-keras libraries for using word-embeddings\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "091d7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dfbfc",
   "metadata": {},
   "source": [
    "### Loading the dataset from local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcbb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\Users\\DELL\\Desktop\\Kaggle Playground\\Disaster prediction using Tweets\\train.csv')\n",
    "test = pd.read_csv(r'C:\\Users\\DELL\\Desktop\\Kaggle Playground\\Disaster prediction using Tweets\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef4d6b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59cb9230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a676653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab8667",
   "metadata": {},
   "source": [
    "### Preprocessing the data \n",
    "#### 1. Removing some columns.\n",
    "#### 2. Removing all the links, stopwords, punctuations, extra spaces present in the data.\n",
    "#### 3. Doing .lower() to bring data in same case.\n",
    "#### 4. Creating word corpus from data, i.e., converting strings into list of words for feeding into Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58188c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns='location', inplace=True)\n",
    "test.drop(columns='location', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c207358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCEPTS STRING => RETURNS STRING\n",
    "def link_hashtag_remover(text):\n",
    "  # reference : https://stackoverflow.com/questions/8376691/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression\n",
    "  # pattern = 'http:[\\/]{2}[a-z].[a-z]*[\\/][a-zA-Z0-9]*|#[a-zA-Z0-9]*|@[a-zA-Z0-9]*'\n",
    "\n",
    "  preprocessed = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "  #re.sub() is used to replace a given pattern with a replacement pattern\n",
    "  return preprocessed\n",
    "\n",
    "\n",
    "# ACCEPTS STRING => RETURNS LIST\n",
    "def stopwords_punct_remover(text):\n",
    "  doc = nlp(text)\n",
    "\n",
    "  preprocessed = []\n",
    "\n",
    "  for token in doc:\n",
    "    if(token.is_stop == False):\n",
    "      if(token.pos_ not in [\"PUNCT\", \"SPACE\"]):\n",
    "        # print(token.text)\n",
    "        preprocessed.append(token.text)\n",
    "\n",
    "  return preprocessed\n",
    "  #return preprocessed\n",
    "\n",
    "# ACCEPTS LIST => RETURNS STRING\n",
    "def lemmatizer(text):\n",
    "  text = \" \".join(text)\n",
    "  str1 = \"\"\n",
    "  doc = nlp(text)\n",
    "  for token in doc:\n",
    "    str1 += token.lemma_\n",
    "    str1 += \" \"\n",
    "  return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "209b4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(text):\n",
    "  text1 = link_hashtag_remover(text)\n",
    "  text2 = stopwords_punct_remover(text1)\n",
    "  text3 = lemmatizer(text2)\n",
    "\n",
    "  return text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a025fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in df['input']:\n",
    "        words = []\n",
    "        doc = nlp(tweet)\n",
    "        for token in doc:\n",
    "            words.append(token.text.lower())\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d213f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['input'] = train['text'].map(lambda x:pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3cb73611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   deed Reason earthquake ALLAH Forgive \n",
       "1                  forest fire near La Ronge Sask Canada \n",
       "2       resident ask shelter place notify officer evac...\n",
       "3       13 000 people receive wildfire evacuation orde...\n",
       "4       got send photo Ruby Alaska smoke wildfire pour...\n",
       "                              ...                        \n",
       "7608        giant crane hold bridge collapse nearby home \n",
       "7609    ahrary control wild fire California northern s...\n",
       "7610               M1 94 01 04 utc 5 km S Volcano Hawaii \n",
       "7611    Police investigate e bike collide car Little P...\n",
       "7612    late Homes Razed Northern California Wildfire ...\n",
       "Name: input, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ad794fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da741da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c27a3",
   "metadata": {},
   "source": [
    "### Word Embeddings using GloVe\n",
    "#### 1. Mapping each word from glove with its suitable numerical word vector of dimensions 50 X 1.\n",
    "#### 2. post padding to make each sentence of equal length by adding 0's at the end.\n",
    "#### 3. Create embedding and convert the text corpus into numerical data and ignoring the words which are not present in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00b6c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GloVe embeddings to map each word with their vectors\n",
    "embeddings_dict = {}\n",
    "with open(r\"C:\\Users\\DELL\\Desktop\\glove.6B\\glove.6B.50d.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e92ba413",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f79e2401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 13182\n"
     ]
    }
   ],
   "source": [
    "# maps each unique word to index, not necessarily in alphabetical order\n",
    "# indexing will start from 1 and not from 0\n",
    "word_index=tokenizer_obj.word_index\n",
    "print('Number of unique words:',len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2944699",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,50))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embeddings_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28c117a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13183, 50)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape # since there are 13182 words each of size 50 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ff553",
   "metadata": {},
   "source": [
    "### Making Neural Network model for Classification of tweet, which contains embedding layer,  dropout layer, LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f53c0c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,50,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4b2074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            659150    \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 50, 50)           0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                29440     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 688,655\n",
      "Trainable params: 29,505\n",
      "Non-trainable params: 659,150\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f25925bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3767,  447,  197, ...,    0,    0,    0],\n",
       "       [ 132,    5,  171, ...,    0,    0,    0],\n",
       "       [1592,  485, 1593, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [2332, 2006,  544, ...,    0,    0,    0],\n",
       "       [  26,  783,  425, ...,    0,    0,    0],\n",
       "       [ 141,  742,  795, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65c58e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   deed Reason earthquake ALLAH Forgive \n",
       "1                  forest fire near La Ronge Sask Canada \n",
       "2       resident ask shelter place notify officer evac...\n",
       "3       13 000 people receive wildfire evacuation orde...\n",
       "4       got send photo Ruby Alaska smoke wildfire pour...\n",
       "                              ...                        \n",
       "7608        giant crane hold bridge collapse nearby home \n",
       "7609    ahrary control wild fire California northern s...\n",
       "7610               M1 94 01 04 utc 5 km S Volcano Hawaii \n",
       "7611    Police investigate e bike collide car Little P...\n",
       "7612    late Homes Razed Northern California Wildfire ...\n",
       "Name: input, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d071fdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3767\n",
      "447\n"
     ]
    }
   ],
   "source": [
    "print(word_index['deed'])\n",
    "print(word_index['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e845b",
   "metadata": {},
   "source": [
    "### Splitting the data into test data and train data using train_test_split function from sklearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a0d2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tweet_pad[:train.shape[0]]\n",
    "test_data = tweet_pad[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bcd8bfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train (6471, 50)\n",
      "Shape of Validation  (1142, 50)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(train_data,train['target'].values,test_size=0.15)\n",
    "print('Shape of train',x_train.shape)\n",
    "print(\"Shape of Validation \",x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3551f8c",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b318ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1618/1618 - 29s - loss: 0.6911 - accuracy: 0.5693 - val_loss: 0.6882 - val_accuracy: 0.5709 - 29s/epoch - 18ms/step\n",
      "Epoch 2/15\n",
      "1618/1618 - 27s - loss: 0.6397 - accuracy: 0.6268 - val_loss: 0.5504 - val_accuracy: 0.7443 - 27s/epoch - 17ms/step\n",
      "Epoch 3/15\n",
      "1618/1618 - 27s - loss: 0.5430 - accuracy: 0.7401 - val_loss: 0.5297 - val_accuracy: 0.7574 - 27s/epoch - 17ms/step\n",
      "Epoch 4/15\n",
      "1618/1618 - 27s - loss: 0.5255 - accuracy: 0.7549 - val_loss: 0.5176 - val_accuracy: 0.7609 - 27s/epoch - 17ms/step\n",
      "Epoch 5/15\n",
      "1618/1618 - 27s - loss: 0.5179 - accuracy: 0.7585 - val_loss: 0.5105 - val_accuracy: 0.7671 - 27s/epoch - 17ms/step\n",
      "Epoch 6/15\n",
      "1618/1618 - 27s - loss: 0.5116 - accuracy: 0.7651 - val_loss: 0.5049 - val_accuracy: 0.7715 - 27s/epoch - 17ms/step\n",
      "Epoch 7/15\n",
      "1618/1618 - 27s - loss: 0.5099 - accuracy: 0.7631 - val_loss: 0.5010 - val_accuracy: 0.7776 - 27s/epoch - 17ms/step\n",
      "Epoch 8/15\n",
      "1618/1618 - 27s - loss: 0.5000 - accuracy: 0.7747 - val_loss: 0.4994 - val_accuracy: 0.7767 - 27s/epoch - 17ms/step\n",
      "Epoch 9/15\n",
      "1618/1618 - 27s - loss: 0.5028 - accuracy: 0.7687 - val_loss: 0.4957 - val_accuracy: 0.7820 - 27s/epoch - 17ms/step\n",
      "Epoch 10/15\n",
      "1618/1618 - 27s - loss: 0.4977 - accuracy: 0.7739 - val_loss: 0.4942 - val_accuracy: 0.7846 - 27s/epoch - 17ms/step\n",
      "Epoch 11/15\n",
      "1618/1618 - 27s - loss: 0.4947 - accuracy: 0.7801 - val_loss: 0.4933 - val_accuracy: 0.7837 - 27s/epoch - 17ms/step\n",
      "Epoch 12/15\n",
      "1618/1618 - 27s - loss: 0.4959 - accuracy: 0.7730 - val_loss: 0.4918 - val_accuracy: 0.7846 - 27s/epoch - 17ms/step\n",
      "Epoch 13/15\n",
      "1618/1618 - 27s - loss: 0.4941 - accuracy: 0.7776 - val_loss: 0.4910 - val_accuracy: 0.7855 - 27s/epoch - 17ms/step\n",
      "Epoch 14/15\n",
      "1618/1618 - 27s - loss: 0.4916 - accuracy: 0.7784 - val_loss: 0.4909 - val_accuracy: 0.7846 - 27s/epoch - 17ms/step\n",
      "Epoch 15/15\n",
      "1618/1618 - 27s - loss: 0.4906 - accuracy: 0.7779 - val_loss: 0.4902 - val_accuracy: 0.7846 - 27s/epoch - 17ms/step\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,batch_size=4,epochs=15,validation_data=(x_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a286c65",
   "metadata": {},
   "source": [
    "### Making the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff9bed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub=pd.read_csv(r'C:\\Users\\DELL\\Desktop\\Kaggle Playground\\Disaster prediction using Tweets\\sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f19ef3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword                                               text\n",
       "0         0     NaN                 Just happened a terrible car crash\n",
       "1         2     NaN  Heard about #earthquake is different cities, s...\n",
       "2         3     NaN  there is a forest fire at spot pond, geese are...\n",
       "3         9     NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4        11     NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "...     ...     ...                                                ...\n",
       "3258  10861     NaN  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
       "3259  10865     NaN  Storm in RI worse than last hurricane. My city...\n",
       "3260  10868     NaN  Green Line derailment in Chicago http://t.co/U...\n",
       "3261  10874     NaN  MEG issues Hazardous Weather Outlook (HWO) htt...\n",
       "3262  10875     NaN  #CityofCalgary has activated its Municipal Eme...\n",
       "\n",
       "[3263 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e8d31",
   "metadata": {},
   "source": [
    "### Preprocessing the test-data, as we need to convert it into model-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "27b71a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['input'] = test['text'].map(lambda x:pipeline(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "208d4c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>text</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>hear earthquake different city stay safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese flee street save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse light Spokane wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>Typhoon Soudelor kill 28 China Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>earthquake safety LOS ANGELES SAFETY fastener ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>storm ri bad hurricane city amp 3others hard h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>Green Line derailment Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>meg issue Hazardous Weather Outlook HWO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>CityofCalgary activate Municipal Emergency Pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword                                               text  \\\n",
       "0         0     NaN                 Just happened a terrible car crash   \n",
       "1         2     NaN  Heard about #earthquake is different cities, s...   \n",
       "2         3     NaN  there is a forest fire at spot pond, geese are...   \n",
       "3         9     NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4        11     NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "...     ...     ...                                                ...   \n",
       "3258  10861     NaN  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...   \n",
       "3259  10865     NaN  Storm in RI worse than last hurricane. My city...   \n",
       "3260  10868     NaN  Green Line derailment in Chicago http://t.co/U...   \n",
       "3261  10874     NaN  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
       "3262  10875     NaN  #CityofCalgary has activated its Municipal Eme...   \n",
       "\n",
       "                                                  input  \n",
       "0                            happen terrible car crash   \n",
       "1             hear earthquake different city stay safe   \n",
       "2         forest fire spot pond geese flee street save   \n",
       "3                    apocalypse light Spokane wildfire   \n",
       "4                Typhoon Soudelor kill 28 China Taiwan   \n",
       "...                                                 ...  \n",
       "3258  earthquake safety LOS ANGELES SAFETY fastener ...  \n",
       "3259  storm ri bad hurricane city amp 3others hard h...  \n",
       "3260                     Green Line derailment Chicago   \n",
       "3261           meg issue Hazardous Weather Outlook HWO   \n",
       "3262  CityofCalgary activate Municipal Emergency Pla...  \n",
       "\n",
       "[3263 rows x 4 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c72c6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = create_corpus(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "42c2a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences=tokenizer_obj.texts_to_sequences(test_corpus)\n",
    "test_tweet_pad=pad_sequences(test_sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "20ff68a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 50)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweet_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b8b67b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 200, 1621,   49, ...,    0,    0,    0],\n",
       "       [ 137,  197,  985, ...,    0,    0,    0],\n",
       "       [ 132,    5,  582, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 724,  480,  289, ...,    0,    0,    0],\n",
       "       [5127,  187,  340, ...,    0,    0,    0],\n",
       "       [1314, 1315,   19, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweet_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c3cee097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['car']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a2256",
   "metadata": {},
   "source": [
    "### Final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0364550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pre = model.predict(test_tweet_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5dfe6be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "750eb255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 1)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "857724c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8249626 ],\n",
       "       [0.8091696 ],\n",
       "       [0.86521316],\n",
       "       [0.8212333 ],\n",
       "       [0.8981525 ]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd954116",
   "metadata": {},
   "source": [
    "### Saving the file that can be directly submitted on kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "642576b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=np.round(y_pre).astype(int).reshape(3263)\n",
    "sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\n",
    "sub.to_csv('submission_word_embeddings.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "91b393a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
